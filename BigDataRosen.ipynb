{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-06T11:47:44.500578600Z",
     "start_time": "2024-02-06T11:47:44.437779400Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vorbereitungen\n",
    "\n",
    "## Dateien in Array schreiben"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b658f46531f0c60"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 Datensätze erfolgreich in datasets-Array eingelesen\n"
     ]
    }
   ],
   "source": [
    "directory = 'datasetsRosen'\n",
    "datasets = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file = h5py.File(directory + '/' + filename)\n",
    "    for key in file.keys():\n",
    "        datasets.append(file[key])\n",
    "        \n",
    "print(f'{len(datasets)} Datensätze erfolgreich in datasets-Array eingelesen')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T11:47:52.115258900Z",
     "start_time": "2024-02-06T11:47:44.513739800Z"
    }
   },
   "id": "7d6fc2c446f0243",
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dateien in Pandas Dataframe schreiben"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c053f87880610890"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_hdf5(file_path):\n",
    "    \"\"\"Read an HDF5 file into a Pandas DataFrame and return it along with attributes.\"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        for keyname in file.keys():\n",
    "            if keyname != \"\":\n",
    "                data_group_name = keyname\n",
    "            else:\n",
    "                print(f\"'data' missing in: {file_path}\")\n",
    "                return None, None\n",
    "\n",
    "        data_group = file[data_group_name]\n",
    "        data_dict = {name: data_group[name][:] for name in data_group.keys()}\n",
    "\n",
    "        # Extract attributes\n",
    "        attributes = {attr: data_group.attrs[attr] for attr in data_group.attrs.keys()}\n",
    "        data_id = attributes.get('id', 'Unknown')\n",
    "\n",
    "        max_length = max(len(values) for values in data_dict.values())\n",
    "        shorter_columns = {name: len(values) for name, values in data_dict.items() if len(values) < max_length}\n",
    "        if shorter_columns:\n",
    "            print(f\"Inconsistency in: {data_id}\")\n",
    "            for name, length in shorter_columns.items():\n",
    "                print(f\" - Column name:: {name}, Length: {length}\")\n",
    "            return None, attributes\n",
    "\n",
    "        data_frame = pd.DataFrame(data_dict)\n",
    "        return data_frame, attributes\n",
    "\n",
    "all_dataframes = {}\n",
    "for file_name in os.listdir('datasetsRosen'):\n",
    "    if file_name.endswith('.h5'):\n",
    "        file_path = os.path.join('datasetsRosen', file_name)\n",
    "        frame, frame_attrs = read_hdf5(file_path)\n",
    "        if frame is not None:\n",
    "            all_dataframes[file_name] = (frame, frame_attrs)\n",
    " print(all_dataframes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T11:48:13.285648800Z",
     "start_time": "2024-02-06T11:47:53.027955500Z"
    }
   },
   "id": "788b8f3931b61528",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000bc7ea-8918-420c-bd13-30e1a6e8dd00.h5: DataFrame Shape: (1000, 6), Attributes: {'configuration': 'Africa', 'id': '000bc7ea-8918-420c-bd13-30e1a6e8dd00', 'instrument': 'Pufferfish'}\n",
      "First 5 lines of the columns:\n",
      "   velocity  defect_channel  distance  magnetization     timestamp  \\\n",
      "0  0.774117             0.0  0.000000      -1.918574  8.643687e+08   \n",
      "1  0.711733             0.0  0.373373       5.128566  8.643692e+08   \n",
      "2  0.882543             0.0  0.746747       1.558591  8.643697e+08   \n",
      "3  0.873818             0.0  1.120120       0.140658  8.643702e+08   \n",
      "4  0.787977             0.0  1.493493       6.028222  8.643707e+08   \n",
      "\n",
      "   wall_thickness  \n",
      "0       25.255805  \n",
      "1       29.027724  \n",
      "2       24.149902  \n",
      "3       22.791865  \n",
      "4       31.121774  \n",
      "\n",
      "\n",
      "006b0b34-324b-496a-8ed7-a40f7d689ee9.h5: DataFrame Shape: (1000, 6), Attributes: {'configuration': 'Europe', 'id': '006b0b34-324b-496a-8ed7-a40f7d689ee9', 'instrument': 'Unicorn'}\n",
      "First 5 lines of the columns:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['velocity'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-60-c607e4917c50>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{name}: DataFrame Shape: {df.shape}, Attributes: {attrs}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"First 5 lines of the columns:\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'velocity'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'defect_channel'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'distance'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'magnetization'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'timestamp'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'wall_thickness'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"\\n\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\IdeaProjects\\DickeDaten\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   2910\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2911\u001B[0m                 \u001B[0mkey\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2912\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_listlike_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mraise_missing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2913\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2914\u001B[0m         \u001B[1;31m# take() does not accept boolean indexers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\IdeaProjects\\DickeDaten\\venv\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_get_listlike_indexer\u001B[1;34m(self, key, axis, raise_missing)\u001B[0m\n\u001B[0;32m   1252\u001B[0m             \u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnew_indexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0max\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reindex_non_unique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1253\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1254\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_read_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mraise_missing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mraise_missing\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1255\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1256\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\IdeaProjects\\DickeDaten\\venv\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_validate_read_indexer\u001B[1;34m(self, key, indexer, axis, raise_missing)\u001B[0m\n\u001B[0;32m   1302\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mraise_missing\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1303\u001B[0m                 \u001B[0mnot_found\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0max\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1304\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{not_found} not in index\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1305\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1306\u001B[0m             \u001B[1;31m# we skip the warning on Categorical\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['velocity'] not in index\""
     ]
    }
   ],
   "source": [
    "for name, (df, attrs) in all_dataframes.items():\n",
    "    print(f\"{name}: DataFrame Shape: {df.shape}, Attributes: {attrs}\")\n",
    "    print(\"First 5 lines of the columns:\")\n",
    "    print(df[['velocity', 'defect_channel', 'distance', 'magnetization', 'timestamp', 'wall_thickness']].head())\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T11:48:13.417867800Z",
     "start_time": "2024-02-06T11:48:13.295180600Z"
    }
   },
   "id": "b73113246ad3b2ec",
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datenbereinigung die Zweite"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df9757eb0956fc11"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_dataframe_from_directory(directory):\n",
    "    dataframes = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.h5'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            frame, frame_attrs = read_hdf5(file_path)\n",
    "            if frame is not None:\n",
    "                dataframes[file_name] = (frame, frame_attrs)\n",
    "    return dataframes\n",
    "    \n",
    "def check_dataframe(dataframes):\n",
    "    newFrame = pd.DataFrame()\n",
    "    \n",
    "    for frame in dataframes.values():\n",
    "        numeric = drop_not_numerical_rows(frame[0])\n",
    "        newFrame = newFrame.append(numeric)\n",
    "    return newFrame\n",
    "\n",
    "def drop_not_numerical_rows(dataframe):\n",
    "    numeric_df = dataframe.apply(pd.to_numeric, errors='coerce')\n",
    "    numeric_df = numeric_df.dropna()\n",
    "    return numeric_df\n",
    "      \n",
    "df = check_dataframe(get_dataframe_from_directory('datasetsRosen'))\n",
    "\n",
    "save_dataframe_to_json(df, 'generated files')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T11:53:26.135552900Z",
     "start_time": "2024-02-06T11:50:29.377406100Z"
    }
   },
   "id": "dd00214d6f5c8185",
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datenbereinigung !!ChatGPT und kacke!!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e37fd3742f000dd9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def check_file(filename):\n",
    "    try:\n",
    "        with h5py.File(filename, 'r') as file:\n",
    "            # Überprüfe hier die Struktur und Integrität der Daten\n",
    "            check_dataset(file, 'defect_channel')\n",
    "            check_dataset(file, 'distance')\n",
    "            check_dataset(file, 'magnetization')\n",
    "            check_dataset(file, 'timestamp')\n",
    "            check_dataset(file, 'velocity')\n",
    "            check_dataset(file, 'wall_thickness')\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Lesen von {filename}: {e}\")\n",
    "\n",
    "def check_dataset(file, dataset_name):\n",
    "    try:\n",
    "        dataset = file['data'][dataset_name][:]\n",
    "        validate_numerical_data(dataset)\n",
    "        check_missing_data(dataset)\n",
    "        # Weitere spezifische Überprüfungen für jedes Dataset können hinzugefügt werden\n",
    "    except KeyError:\n",
    "        print(f\"Fehler: Dataset '{dataset_name}' nicht vorhanden.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Überprüfung von '{dataset_name}': {e}\")\n",
    "\n",
    "def validate_numerical_data(dataset):\n",
    "    # Beispiel: Überprüfe, ob alle Werte positiv sind\n",
    "    if (dataset < 0).any():\n",
    "        print(\"Fehler: Negative Werte in den Daten.\")\n",
    "\n",
    "# Weitere Validierungsbedingungen können hier hinzugefügt werden\n",
    "\n",
    "def check_missing_data(dataset):\n",
    "    # Beispiel: Überprüfe, ob NaN-Werte in den Daten vorhanden sind\n",
    "    if np.isnan(dataset).any():\n",
    "        print(\"Fehler: NaN-Werte in den Daten.\")\n",
    "\n",
    "# Weitere Überprüfungen auf fehlende Daten können hier hinzugefügt werden\n",
    "\n",
    "def log_result(filename, result):\n",
    "    with open(\"log.txt\", \"a\") as log_file:\n",
    "        log_file.write(f\"{filename}: {result}\\n\")\n",
    "\n",
    "# Iteriere über alle Dateien und prüfe jede einzelne\n",
    "folder_path = \"datasetsRosen\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".h5\"):\n",
    "        check_file(os.path.join(folder_path, file_name))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-06T11:48:13.416867700Z"
    }
   },
   "id": "43081c0b5efc2349",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datenüberprüfung\n",
    "\n",
    "## Welche Konfigurationen und Instrumente gibt es in den Datensätzen?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "808cf18da2eecff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "confs_directory = 'datasetsRosen'\n",
    "configurations = []\n",
    "instruments = []\n",
    "configAmount = 0\n",
    "instAmount = 0\n",
    "\n",
    "for idx, filename in enumerate(os.listdir(confs_directory)):\n",
    "    file = h5py.File(confs_directory + '/' + filename)\n",
    "    if 'data' in file:\n",
    "        configName = file['data'].attrs['configuration']\n",
    "        if file['data'].attrs['configuration']:\n",
    "            configAmount += 1\n",
    "        instrumentName = file['data'].attrs['instrument']\n",
    "        if file['data'].attrs['instrument']:\n",
    "            instAmount += 1\n",
    "        if all(configName not in x for x in configurations):\n",
    "            configurations.append(configName)\n",
    "        if all(instrumentName not in x for x in instruments):\n",
    "            instruments.append(instrumentName)\n",
    "    if 'Daten' in file:\n",
    "        configName = file['Daten'].attrs['configuration']\n",
    "        if file['Daten'].attrs['configuration']:\n",
    "            configAmount += 1\n",
    "        instrumentName = file['Daten'].attrs['instrument']\n",
    "        if file['Daten'].attrs['instrument']:\n",
    "            instAmount += 1\n",
    "        if all(configName not in x for x in configurations):\n",
    "            configurations.append(configName)\n",
    "        if all(instrumentName not in x for x in instruments):\n",
    "            instruments.append(instrumentName)\n",
    "\n",
    "print(f'Menge an Configurations: {configAmount}')\n",
    "print('Configurations: ', configurations)\n",
    "print(f'Menge an Instruments: {instAmount}')\n",
    "print('Instruments: ', instruments)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T11:48:13.425625600Z",
     "start_time": "2024-02-06T11:48:13.422591Z"
    }
   },
   "id": "7a2895e8f00f5b23",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wie viele falsche / fehlende Datensätze gibt es?\n",
    "\n",
    "Prüft auf: \n",
    "    Negative Werte,\n",
    "    Fehlende Datensätze / Falsche Key Bezeichnung"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1562b845253cd3f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# defect_channel, distance, magnetization, timestamp, velocity, wall_thickness\n",
    "attribute = (\"magnetization\"\n",
    "             \"\") # attribute ersetzen um verschiedene Attribute zu testen\n",
    "count_negativeValue = 0\n",
    "negativeValuesIdx = []\n",
    "count_noDataset = 0\n",
    "noDatasetIdx = []\n",
    "eastereggCount = 0\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    # print(dataset[attribute][...])\n",
    "    if attribute in dataset:\n",
    "        if b'Easteregg :)' not in dataset[attribute][...]:\n",
    "            velocity_data = dataset[attribute][...].astype(float)\n",
    "            if (velocity_data < 0).any():\n",
    "                count_negativeValue += 1\n",
    "                negativeValuesIdx.append(idx)\n",
    "        else:\n",
    "            eastereggCount += 1\n",
    "    else:\n",
    "        count_noDataset += 1\n",
    "        noDatasetIdx.append(idx)\n",
    "        \n",
    "print(f\"---------------------------------------------- {attribute} Analyse ----------------------------------------------\")\n",
    "print(f\"Anzahl negative Werte: {count_negativeValue}\\n Indizes: {negativeValuesIdx}\\n\")\n",
    "print(f\"Anzahl fehlender {attribute} Datensätze: {count_noDataset}\\n Indizes: {noDatasetIdx}\")\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(f\"Anzahl Eastereggs: {eastereggCount}\")\n",
    "print(datasets[1959].keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T11:48:13.504249800Z",
     "start_time": "2024-02-06T11:48:13.426617500Z"
    }
   },
   "id": "6309c04332ea3cb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pandas Dataframe als Dateien speichern"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ea59cbfbbd15269"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def save_dataframe_to_json(dataframes, output_directory):\n",
    "    json_string = dataframes.to_json(orient=\"table\")\n",
    "    file_path = os.path.join(output_directory, 'data' + \".json\")\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(json_string)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-06T11:48:13.431622400Z"
    }
   },
   "id": "65d81e7eb2e5cba4",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
